X <- тренировочные входные данные
Y <- тренировочные выходные данные

EP <- количество эпох
BS <- размер батча
LR <- размер шага обучения

NNL <- массив слоев нейронной сети

Для i на промежутке(EP):
    (x, y) <- случайные примеры из X, Y размером BS

    для xi, yi на массиве (x, y):
        input_data <- xi
        для каждого слоя layer в массиве NNL:
            input_data <- layer.прямой_проход(input_data)
        
        loss <- yi

        для каждого слоя layer в массиве NNL: в обратном порядке:
            loss, delta_w <- layer.обратный_проход(loss)

            если layer.наличие_параметров = Истина:
                layer.w += delta_w * LR

                

             